{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5b66e5-525e-44f9-a798-2c913bf3c890",
   "metadata": {},
   "source": [
    "# AI Based Chatbot to answer DevOps Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e48cd-9f9a-4f30-8551-8b89f096df8b",
   "metadata": {},
   "source": [
    "### We intend to create LLM based AI Chatbot that would answer the questions about Linux\n",
    "\n",
    "It is simple LLM bot that answers the questions only on the trained dataset unlike RAG. We will train the data on the corpus of few Linux books like Linux Bible, etc. The vectorstore to store the learnings is FAISS database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada4d1c-f060-492e-abe7-253d07cb5eb6",
   "metadata": {},
   "source": [
    "### Technology Used\n",
    "1. __LLM__: meta-llama/Llama-2-7b-chat-hf [https://huggingface.co/meta-llama/Llama-2-7b-chat-hf]\n",
    "2. __VectorStore__: ___FAISS___ => FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other [https://ai.meta.com/tools/faiss/]\n",
    "3. __Embeddings__: ___sentence-transformers/all-mpnet-base-v2___ [https://huggingface.co/sentence-transformers/all-mpnet-base-v2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14f491-89b7-4635-a774-ad4124301bdc",
   "metadata": {},
   "source": [
    "### Installing dependencies using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60274355-4872-4b54-a179-eedf91cb8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ bitsandbytes --quiet\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b659a8-e5c0-4198-bfba-0fd2b5b38b27",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad320cff-71a5-49a4-8fe8-a08847ab2198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "from langchain.globals import set_debug, set_verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e730515-5575-4d87-93e1-870ee51b902c",
   "metadata": {},
   "source": [
    "### Neccessary Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f2c08a-0b0c-4250-9be4-48878e20c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fdf9f-8553-4601-b353-ab997bf1de45",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e5ec990-bd43-4d99-9ccb-77c07314133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "def load_llama(model_id: Any) -> Any:\n",
    "    print(f\"Loading the model {model_id}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                 pad_token_id=tokenizer.eos_token_id\n",
    "                                                )\n",
    "    model.to(device)\n",
    "    llama_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=True,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        task=\"text-generation\",  # LLM task\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=llama_pipeline)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dba731-07c7-40eb-b46d-9927482f184d",
   "metadata": {},
   "source": [
    "### Logging in to HuggingFace repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dfc8dd9-8e20-44c7-80e8-c87f48609fa3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "def huggingface_login() -> None:\n",
    "     os.environ[\"HF_TOKEN\"] = \"YOUR_HUGGINGFACE_TOKEN\"\n",
    "     print(\"------------------------------------\")\n",
    "     print(\"Huggingface login\")\n",
    "     huggingface_hub.login(os.environ[\"HF_TOKEN\"])\n",
    "     print(\"------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3d910-f86d-41f0-a9f7-287fa5b9dcf8",
   "metadata": {},
   "source": [
    "### Setting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce913b80-37ad-4124-9af6-cc2954e6c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def set_embeddings(model_name: Any) -> Any:\n",
    "     print(f\"\\n------------------------------------\")\n",
    "     print(f\"Setting the embeddings\")\n",
    "     embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "     print(f\"Embeddings set successfully\")\n",
    "     print(f\"------------------------------------\\n\")\n",
    "     return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e33613-5636-42a5-8399-58412df8add3",
   "metadata": {},
   "source": [
    "### Loading the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49965f37-b387-43b0-9661-31e885c4eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "\n",
    "def load_documents(local_directory_path: str) -> Any:\n",
    "    # For PDF files\n",
    "    print(f\"\\n------------------------------------\")\n",
    "    print(f\"Loading PDFs from {local_directory_path}\")\n",
    "    loader = DirectoryLoader(local_directory_path,\n",
    "                                glob='*.pdf',\n",
    "                                loader_cls=PyPDFLoader)\n",
    "    print(loader)\n",
    "    documents = loader.load()\n",
    "    print(f\"Documents Loaded\")\n",
    "    print(f\"------------------------------------\\n\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518592fb-914a-4188-86ff-37dcab04e298",
   "metadata": {},
   "source": [
    "### Processing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8c5557-153b-412e-902d-f71c5238f7a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def process_documents(documents: Any) -> Any:\n",
    "    print(f\"\\n------------------------------------\")\n",
    "    print(f\"Processing the documents\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                                    chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Documents processed\")\n",
    "    print(f\"------------------------------------\\n\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ea0b9-214d-4a57-a6a6-18ccdbb69911",
   "metadata": {},
   "source": [
    "### Saving to FAISS Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5b070b0-ae52-437c-bea4-79e5505fd266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def save_to_vectorstore(texts: Any, embeddings: Any, vectorestore_path: str) -> Any:\n",
    "     print(f\"\\n------------------------------------\")\n",
    "     print(f\"Saving the vectorestore to {vectorestore_path}\")\n",
    "     vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "     vectorstore.save_local(vectorestore_path)\n",
    "     print(f\"Vectore DB stored at {vectorestore_path}\")\n",
    "     print(f\"------------------------------------\\n\")\n",
    "     return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f5257-202d-4da8-a947-7d112ef43f47",
   "metadata": {},
   "source": [
    "### Setting the custom prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf42d62-54f3-47ea-8d08-5fe9a8949385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "custom_prompt_template = \"\"\"Use the following information to answer the user's question.\n",
    "In case you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt() -> Any:\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    print(\"Setting the custom prompt\")\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f114cf-8bb7-4f37-b541-4a55be367da4",
   "metadata": {},
   "source": [
    "### Retrieval QA Chain function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3517f14-be57-4dcc-860d-cec5ba339da4",
   "metadata": {},
   "source": [
    "The RetrievalQAChain is a chain that combines a Retriever and a QA chain (described above). It is used to retrieve documents from a Retriever and then use a QA chain to answer a question based on the retrieved documents. [Read this for more info on RetrievalQA](https://js.langchain.com/docs/modules/chains/popular/vector_db_qa_legacy#:~:text=The%20RetrievalQAChain%20is%20a%20chain,based%20on%20the%20retrieved%20documents.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ece2e36-3349-4978-a26c-cf33e35b8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def retrieval_qa_chain(llm: Any, vectorstore: Any) -> Any:\n",
    "     chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                         chain_type='stuff',\n",
    "                                         retriever=vectorstore.as_retriever(search_kwargs={'k': 2}), \n",
    "                                         return_source_documents=True,\n",
    "                                         chain_type_kwargs={'prompt': set_custom_prompt()},\n",
    "                                         verbose=False\n",
    "                                         )\n",
    "     return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f26d0a-a7ee-4b2a-8925-ba54fe0cf488",
   "metadata": {},
   "source": [
    "### Chatbot Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a5f23e-a610-4641-b917-22daf5c33cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(llm: Any, vectorstore: Any) -> Any:\n",
    "    chain = retrieval_qa_chain(llm, vectorstore)\n",
    "    exit_conditions = (\"exit\", \"bye\", \"quit\", \":q\")\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        \n",
    "        if user_input.lower() in exit_conditions:\n",
    "            print(\"Chatbot: Thanks!\")\n",
    "            break\n",
    "        result = chain({\"query\": user_input})\n",
    "\n",
    "        response = result[\"result\"]\n",
    "        \n",
    "        print(\"Chatbot: \", response)\n",
    "        print(\"--------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71222f58-9a6a-4ca4-a510-61c3894eeaf2",
   "metadata": {},
   "source": [
    "### Sample testing. Having the debugger output on for understanding the flow of responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa2c1b-2831-4f49-ad2b-ede04056ea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Huggingface login\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/sagemaker-user/.cache/huggingface/token\n",
      "Login successful\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Loading PDFs from /home/sagemaker-user/content/corpus\n",
      "<langchain.document_loaders.directory.DirectoryLoader object at 0x7f2cbdd4dc60>\n",
      "Documents Loaded\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Processing the documents\n",
      "Documents processed\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Setting the embeddings\n",
      "Embeddings set successfully\n",
      "------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "Saving the vectorestore to /home/sagemaker-user/content/vectorstore/\n",
      "Vectore DB stored at /home/sagemaker-user/content/vectorstore/\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_hf_token()\n",
    "    documents = load_documents(\"/home/sagemaker-user/content/corpus\")\n",
    "    texts = process_documents(documents)\n",
    "    embeddings = set_embeddings('sentence-transformers/all-mpnet-base-v2')\n",
    "    vectorstore = save_to_vectorstore(texts, embeddings, \"/home/sagemaker-user/content/vectorstore/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "290e518f-4311-46f9-8408-3d0bf0acdf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801d033e136446eb81732c962cfdff34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the custom prompt\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  What is linux\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What is linux\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is linux\",\n",
      "  \"context\": \"other hand, was developed in a different context. Linux is a PC version of the Unix operating system that has been used for decades on mainframes and minicomputers and is currently the system of choice for network servers and workstations. Linux brings the \\nspeed, efficiency, scalability, and flexibility of Unix to your PC, taking advantage of all the \\ncapabilities that PCs can now provide.\\nTechnically, Linux consists of the operating system program, referred to as the kernel,\\n\\nLinux ������\\n \\n������������������\\n \\n������������\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following information to answer the user's question.\\nIn case you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: other hand, was developed in a different context. Linux is a PC version of the Unix operating system that has been used for decades on mainframes and minicomputers and is currently the system of choice for network servers and workstations. Linux brings the \\nspeed, efficiency, scalability, and flexibility of Unix to your PC, taking advantage of all the \\ncapabilities that PCs can now provide.\\nTechnically, Linux consists of the operating system program, referred to as the kernel,\\n\\nLinux ������\\n \\n������������������\\n \\n������������\\nQuestion: What is linux\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:HuggingFacePipeline] [16.91s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Linux is an open-source operating system that is based on the Unix operating system and is designed to be fast, efficient, scalable, and flexible. It is typically used on servers and workstations, but can also be used on personal computers.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [16.92s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Linux is an open-source operating system that is based on the Unix operating system and is designed to be fast, efficient, scalable, and flexible. It is typically used on servers and workstations, but can also be used on personal computers.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [16.92s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"Linux is an open-source operating system that is based on the Unix operating system and is designed to be fast, efficient, scalable, and flexible. It is typically used on servers and workstations, but can also be used on personal computers.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [16.95s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "Chatbot:  Linux is an open-source operating system that is based on the Unix operating system and is designed to be fast, efficient, scalable, and flexible. It is typically used on servers and workstations, but can also be used on personal computers.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"exi\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"exi\",\n",
      "  \"context\": \"expressions, evaluating, 133–135, 443,\\n620, 647–650\\next2 filesystem\\nchecking and repairing, 125\\ndebugging, 107–110\\nformatting devices as, 288–290\\nlabel for, displaying, 127\\nprinting block and superblock\\ninformation, 124\\nresizing, 356\\nstoring disaster recovery data\\nfor, 126\\ntuning parameters of, 457–460\\next3 filesystem, 16\\ndebugging, 107–110\\nformatting devices as, 291\\nprinting block and superblock\\ninformation, 124\\nextended Internet services\\ndaemon, 490–493\\nextended regular expressions, searching\\nwith, 128\\nExtensible Filesystem (see XFS)\\nextension command, logrotate, 254\\nextension() function, gawk, 740\\nExterior Gateway Protocol (EGP), 25\\neXternal Data Representation (XDR), 32\\n\\n266, 271\\nWindows Ubuntu Installer (Wubi), 24, 42, 53Windows Vista, 27–28, 33, 36Windows XP, 28–29, 33, 36\\nWinzip program, 299wireless card compatibility, 34wireless networks, 150–151, 152–153wireless signal meter gadget, 290WMV (Windows Media Video) video format, \\n266, 271\\nword processing applications, 201, 219. See also \\nWriter, OpenOffi  ce.org\\nWorkspace Switcher, 72, 73\\nworkspaces, moving programs between, 72, 73wrapping of command lines, 3write permissions, 116Writer, OpenOffi  ce.org, 213, 214, 215, \\n218–219, 293\\nw32codec, 266Wubi (Windows Ubuntu Installer), 24, 42, 53WWW services, 360\\n• X •\\nX Window System (X), 110, 111Xandros specialized distribution, 20X-Chat IRC program, 181XFS fi  lesystem, 48\\nX.org.0.log fi  le, 362\\n• Y •\\nYouTube, 267, 293yum software installer, 282–283, 298, 300, 357\\n• Z •\\n.z fi les, 299\\n.zip fi  les, 299\\nzip program, 299, 411\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following information to answer the user's question.\\nIn case you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: expressions, evaluating, 133–135, 443,\\n620, 647–650\\next2 filesystem\\nchecking and repairing, 125\\ndebugging, 107–110\\nformatting devices as, 288–290\\nlabel for, displaying, 127\\nprinting block and superblock\\ninformation, 124\\nresizing, 356\\nstoring disaster recovery data\\nfor, 126\\ntuning parameters of, 457–460\\next3 filesystem, 16\\ndebugging, 107–110\\nformatting devices as, 291\\nprinting block and superblock\\ninformation, 124\\nextended Internet services\\ndaemon, 490–493\\nextended regular expressions, searching\\nwith, 128\\nExtensible Filesystem (see XFS)\\nextension command, logrotate, 254\\nextension() function, gawk, 740\\nExterior Gateway Protocol (EGP), 25\\neXternal Data Representation (XDR), 32\\n\\n266, 271\\nWindows Ubuntu Installer (Wubi), 24, 42, 53Windows Vista, 27–28, 33, 36Windows XP, 28–29, 33, 36\\nWinzip program, 299wireless card compatibility, 34wireless networks, 150–151, 152–153wireless signal meter gadget, 290WMV (Windows Media Video) video format, \\n266, 271\\nword processing applications, 201, 219. See also \\nWriter, OpenOffi  ce.org\\nWorkspace Switcher, 72, 73\\nworkspaces, moving programs between, 72, 73wrapping of command lines, 3write permissions, 116Writer, OpenOffi  ce.org, 213, 214, 215, \\n218–219, 293\\nw32codec, 266Wubi (Windows Ubuntu Installer), 24, 42, 53WWW services, 360\\n• X •\\nX Window System (X), 110, 111Xandros specialized distribution, 20X-Chat IRC program, 181XFS fi  lesystem, 48\\nX.org.0.log fi  le, 362\\n• Y •\\nYouTube, 267, 293yum software installer, 282–283, 298, 300, 357\\n• Z •\\n.z fi les, 299\\n.zip fi  les, 299\\nzip program, 299, 411\\nQuestion: exi\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:HuggingFacePipeline] [11.14s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I don't know the answer to that question.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [11.15s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I don't know the answer to that question.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [11.15s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"I don't know the answer to that question.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [11.18s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "Chatbot:  I don't know the answer to that question.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Thanks!\n"
     ]
    }
   ],
   "source": [
    "set_debug(True)\n",
    "set_verbose(True)\n",
    "chatbot(load_llama(\"meta-llama/Llama-2-7b-chat-hf\"), vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd94859-b105-4881-96fd-da14eb5927ef",
   "metadata": {},
   "source": [
    "## Final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d30bc09-91ad-47b1-9af0-30039c411652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f4f05dd90446e8980c42e40699eb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the custom prompt\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  What is cloud computing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Cloud computing refers to the practice of deploying applications and services on the internet with a cloud provider.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  What is AWS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  AWS stands for Amazon Web Services. It is a cloud computing platform offered by Amazon that provides a range of services for computing, storage, networking, database, analytics, machine learning, and more.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  What are some of the major services in AWS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Some of the major services in AWS include:\n",
      "\n",
      "* Compute Services (EC2, Lambda, Elastic Beanstalk)\n",
      "* Storage Services (S3, EBS, Elastic File System)\n",
      "* Database Services (RDS, DynamoDB, Redshift)\n",
      "* Security, Identity & Compliance Services (IAM, Cognito, Certificate Manager)\n",
      "* Application Services (API Gateway, AppSync, CloudFront)\n",
      "* Analytics Services (Redshift, QuickSight, Athena)\n",
      "* Machine Learning Services (SageMaker, Comprehend, Rekognition)\n",
      "* Networking & Connectivity Services (VPC, Elastic IP, Direct Connect)\n",
      "\n",
      "Note: This list is not exhaustive and is based on my understanding of the question.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  What is Devops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  DevOps is a set of principles and practices that aims to bring together software developers and operations teams to collaborate and automate the software delivery process, with the goal of faster and more reliable end-to-end delivery of software systems to end customers.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Tell me about CI/CD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  CI/CD stands for Continuous Integration/Continuous Deployment. It refers to the practice of integrating and testing code changes frequently, and automatically deploying those changes to production after they pass automated tests. The goal of CI/CD is to improve the efficiency and reliability of software delivery by reducing the time and effort required to move code from development to production.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  list prominent ci/cd tools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Jenkins\n",
      "Bamboo\n",
      "GoCD\n",
      "Team City\n",
      "Electric Cloud\n",
      "\n",
      "Please note that the list of prominent CI/CD tools is not exhaustive, and there are many other tools available in the market.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  10 important commands in linux\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  \n",
      "10 important commands in Linux are:\n",
      "\n",
      "1. ls (list files)\n",
      "2. cd (change directory)\n",
      "3. pwd (print working directory)\n",
      "4. mv (move or rename files)\n",
      "5. rm (remove files)\n",
      "6. cp (copy files)\n",
      "7. mkdir (create directory)\n",
      "8. rmdir (remove directory)\n",
      "9. echo (print text to a file)\n",
      "10. cat (concatenate and display files)\n",
      "\n",
      "Note: These are some of the most commonly used commands in Linux, but there are many more available.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  What is the difference between sudo -i and sudo su -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  sudo -i and sudo su - are both used to run commands as a different user, but they have slightly different usage and behavior.\n",
      "\n",
      "sudo -i allows you to run a command as a different user without a password, while sudo su - requires a password to switch users.\n",
      "\n",
      "In general, it's recommended to use sudo -i when you need to run a command as a different user without a password, and to use sudo su - when you want to switch users and run a command with a password.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what is iphone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  iPhone is a brand of smartphones developed by Apple Inc.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Who is the prime minister of INdia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  I don't know the answer to that question. Sanjeev is a cloud and DevOps expert, not a political analyst.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Tell me something about networking in cloud\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  The cloud has had a significant impact on networking, as it has enabled the emergence of new networking models and technologies. For example, AWS and OpenStack provide networking services out of the box, such as VPCs, subnets, and security groups. These services have made networking a commodity much like infrastructure, allowing developers to consume networking resources on demand. Additionally, cloud networking has enabled the use of software-defined networking (SDN) and network functions virtualization (NFV) technologies, which can be used to programmatically control and automate network functions.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Thanks!\n"
     ]
    }
   ],
   "source": [
    "chatbot(load_llama(\"meta-llama/Llama-2-7b-chat-hf\"), vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d443ad-44e0-4026-b8ae-14da2a30128f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
